# Airbyte Infrastructure

This Terraform module deploys Airbyte on AWS EKS with PostgreSQL RDS.

## Architecture

- **VPC**: Custom VPC with public and private subnets across 2 AZs
- **EKS**: Kubernetes cluster v1.28 for running Airbyte
- **RDS**: PostgreSQL 15.4 database for Airbyte metadata
- **S3**: Storage for logs and artifacts
- **NAT Gateway**: Single NAT gateway for dev (cost optimization)

## Network Layout (Dev)

- VPC CIDR: `10.0.0.0/16`
- Public Subnets: `10.0.1.0/24`, `10.0.2.0/24`
- Private Subnets: `10.0.10.0/24`, `10.0.11.0/24`
- Availability Zones: `us-west-2a`, `us-west-2b`

## Components

### EKS Cluster
- Version: 1.28
- Node type: t3.medium
- Node count: 2 (autoscaling 1-4)
- Add-ons: EBS CSI, VPC CNI, CoreDNS, kube-proxy

### RDS PostgreSQL
- Instance: db.t3.medium
- Storage: 50GB GP3
- Version: PostgreSQL 15.4
- Backup retention: 7 days
- Multi-AZ: Disabled for dev

### Airbyte
- Deployed via Helm chart
- Components: webapp, server, worker (x2), temporal
- External database: RDS PostgreSQL
- Storage: S3

## Prerequisites

Before deploying:
1. Ensure AWS credentials are configured in GitHub secrets
2. Update `tf/environments/dev-common.tfvars` with AWS region (if not already present)
3. Review and customize `tf/environments/dev-airbyte.tfvars` with desired configuration

## Deployment

### Via GitHub Actions (Recommended)

1. **Plan (CI):**
   - Go to Actions → "CI Terraform"
   - Run workflow with: environment=`dev`, service=`airbyte`
   - Review plan output in PR or workflow logs

2. **Apply (Deploy):**
   - Go to Actions → "Deploy Airbyte"
   - Run workflow with: environment=`dev`
   - Monitor progress (15-20 minutes for EKS cluster creation)

### Manual Deployment

```bash
# Initialize
docker compose -f docker/terraform-runner/compose.yaml run \
  --entrypoint tf-init.sh terraform dev airbyte

# Plan
docker compose -f docker/terraform-runner/compose.yaml run \
  --entrypoint tf-plan.sh terraform dev airbyte

# Apply
docker compose -f docker/terraform-runner/compose.yaml run \
  --entrypoint tf-apply.sh terraform dev airbyte
```

## Accessing Airbyte

### Get EKS Cluster Access

```bash
# Configure kubectl
aws eks update-kubeconfig --region us-west-2 --name airbyte-dev-eks

# Verify pods are running
kubectl get pods -n airbyte

# Verify services
kubectl get svc -n airbyte
```

### Get Airbyte UI URL

```bash
# Get LoadBalancer hostname
kubectl get svc -n airbyte airbyte-airbyte-webapp-svc \
  -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'

# Access Airbyte UI
# Navigate to: http://<loadbalancer-hostname>:8000
```

Or use Terraform output:

```bash
terraform output airbyte_loadbalancer_instructions
```

## Cost Estimation

**Dev Environment Monthly Cost: ~$262**

| Resource | Quantity | Monthly Cost |
|----------|----------|--------------|
| EKS control plane | 1 | $73 |
| EC2 t3.medium nodes | 2 | $60 |
| NAT gateway | 1 | $45 |
| RDS db.t3.medium | 1 | $60 |
| RDS storage (50GB) | 50GB | $6 |
| S3 storage | ~100GB | $2 |
| Load balancer | 1 | $16 |

### Cost Optimization for Dev
- Single NAT Gateway (vs 2 saves $32/mo)
- t3.medium instances (vs larger types)
- No Multi-AZ RDS (saves ~$60/mo)
- Autoscaling min=1 for off-hours

### Production Considerations
For production, update `prod-airbyte.tfvars` to use:
- NAT Gateway per AZ for high availability
- Larger instance types (t3.large or m5.large)
- Multi-AZ RDS
- Increased node counts
- Consider Reserved Instances for cost savings

## Security Considerations

### Network Security
- EKS nodes and RDS deployed in private subnets only
- Security groups restrict traffic between components
- NAT gateway provides controlled outbound internet access
- No direct internet access to RDS or EKS nodes

### IAM Security
- EKS cluster and nodes use dedicated IAM roles
- IRSA (IAM Roles for Service Accounts) for Airbyte service account
- No static AWS credentials in pods
- Least privilege policies

### Secrets Management
- Database password generated by Terraform using `random_password`
- Stored securely in AWS Secrets Manager
- Referenced in Kubernetes secrets
- Never logged or exposed in outputs (marked sensitive)

## Troubleshooting

### Common Issues

**1. EKS cluster creation timeout**
- EKS cluster takes 15-20 minutes to provision
- Check CloudWatch logs: `/aws/eks/airbyte-dev-eks/cluster`
- Verify VPC/subnet configuration and tags

**2. RDS connection failures**
- Verify security group allows port 5432 from EKS nodes
- Check RDS is in `available` state: `aws rds describe-db-instances`
- Verify database credentials in Kubernetes secret

**3. Airbyte pods not starting**
- Check pod logs: `kubectl logs -n airbyte <pod-name>`
- Verify database connection from pod
- Check PVC binding (requires EBS CSI driver)
- Review pod events: `kubectl describe pod -n airbyte <pod-name>`

**4. LoadBalancer not getting external IP**
- Check AWS Load Balancer Controller status
- Verify subnet tags include `kubernetes.io/role/elb`
- Check service annotations in airbyte.tf
- Review LoadBalancer events: `kubectl describe svc -n airbyte airbyte-airbyte-webapp-svc`

### Getting Help

**Retrieve RDS password:**
```bash
aws secretsmanager get-secret-value \
  --secret-id <secret-arn-from-output> \
  --query SecretString \
  --output text | jq -r .password
```

**Get EKS cluster info:**
```bash
aws eks describe-cluster --name airbyte-dev-eks --region us-west-2
```

**Check Kubernetes logs:**
```bash
kubectl logs -n airbyte <pod-name> --tail=100 --follow
```

**Get all Airbyte resources:**
```bash
kubectl get all -n airbyte
```

## Maintenance

### Regular Tasks

**Database Backups:**
- Automated daily backups (7-day retention)
- Manual snapshot before major changes:
  ```bash
  aws rds create-db-snapshot \
    --db-instance-identifier airbyte-dev-db-xxx \
    --db-snapshot-identifier airbyte-dev-manual-snapshot-$(date +%Y%m%d)
  ```

**Scaling:**
Update variables in `dev-airbyte.tfvars`:
- `eks_node_desired_size` - Adjust number of nodes
- `db_instance_class` - Scale RDS instance

Then apply changes:
```bash
docker compose -f docker/terraform-runner/compose.yaml run \
  --entrypoint tf-apply.sh terraform dev airbyte
```

**Updates:**
- **EKS**: Update `eks_cluster_version` variable
- **Airbyte**: Update `airbyte_version` variable
- Always review change logs before updating

### Monitoring

**CloudWatch Metrics:**
- EKS cluster metrics: AWS Console → CloudWatch → Container Insights
- RDS metrics: AWS Console → RDS → Monitoring
- S3 usage: AWS Console → S3 → Metrics

**Kubernetes Monitoring:**
```bash
# Resource usage
kubectl top nodes
kubectl top pods -n airbyte

# Get cluster events
kubectl get events -n airbyte --sort-by='.lastTimestamp'
```

## Disaster Recovery

### Backup Strategy
- **RDS**: Automated daily backups with 7-day retention
- **EKS**: Stateless workloads, can be recreated from Terraform
- **S3**: Versioning enabled for logs bucket

### Recovery Procedures

**Restore from RDS backup:**
```bash
# List available snapshots
aws rds describe-db-snapshots \
  --db-instance-identifier airbyte-dev-db-xxx

# Restore from snapshot
aws rds restore-db-instance-from-db-snapshot \
  --db-instance-identifier airbyte-dev-db-restored \
  --db-snapshot-identifier <snapshot-id>
```

**Recreate EKS cluster:**
```bash
# Destroy and recreate (will preserve RDS data)
docker compose -f docker/terraform-runner/compose.yaml run \
  --entrypoint sh terraform -c "cd /usr/src/tf/airbyte && terraform destroy -target=module.eks"

docker compose -f docker/terraform-runner/compose.yaml run \
  --entrypoint tf-apply.sh terraform dev airbyte
```

## Success Criteria

Deployment is successful when:
1. ✅ All Terraform resources created without errors
2. ✅ EKS cluster healthy with nodes in `Ready` state
3. ✅ RDS instance in `available` state
4. ✅ All Airbyte pods running (webapp, server, worker, temporal)
5. ✅ LoadBalancer has external hostname assigned
6. ✅ Airbyte UI loads at http://&lt;loadbalancer&gt;:8000
7. ✅ Setup wizard completes successfully
8. ✅ First connection test succeeds

## Rollback Plan

If deployment fails:

1. **Review errors:**
   - Check Terraform plan output
   - Review GitHub Actions logs
   - Check AWS Console for resource status

2. **Manual cleanup if needed:**
   ```bash
   docker compose -f docker/terraform-runner/compose.yaml run \
     --entrypoint sh terraform -c "cd /usr/src/tf/airbyte && terraform destroy"
   ```

3. **Fix issues and redeploy**

## Next Steps

### After Dev Environment is Stable

1. Create `nonprod-airbyte.tfvars` with similar configuration
2. Create `prod-airbyte.tfvars` with production settings:
   - Multi-AZ RDS (`db_multi_az = true`)
   - Multiple NAT gateways (`single_nat_gateway = false`)
   - Larger instance types
   - Increased node counts

3. Add production features:
   - SSL/TLS certificate with ACM
   - Configure DNS with Route53
   - Set up monitoring dashboards
   - Configure alerts and notifications
   - Document runbooks

4. Implement advanced features:
   - VPN or PrivateLink for secure admin access
   - AWS WAF for LoadBalancer protection
   - Enhanced monitoring with Prometheus/Grafana
   - Automated backups to separate AWS account

Building the terraform-runner Docker Container

  The Dockerfile accepts AWS credentials as build arguments. Build it from the project root:

  docker build \
    --build-arg AWS_ACCESS_KEY_ID=your-access-key-id \
    --build-arg AWS_SECRET_ACCESS_KEY=your-secret-access-key \
    --build-arg AWS_REGION=us-west-2 \
    -t terraform-runner:latest \
    -f docker/terraform-runner/Dockerfile \
    docker/terraform-runner

  Running the Terraform Deployment

  The container has three scripts available (tf-init.sh, tf-plan.sh, tf-apply.sh) that accept two arguments: environment and service name.

  Option 1: Run with docker run

  # Plan the deployment
  docker run --rm \
    -v $(pwd)/tf:/usr/src/tf \
    terraform-runner:latest \
    tf-plan.sh dev airbyte

  # Apply the deployment
  docker run --rm \
    -v $(pwd)/tf:/usr/src/tf \
    terraform-runner:latest \
    tf-apply.sh dev airbyte

  Option 2: Interactive shell

  For more control, run an interactive shell:

  docker run -it --rm \
    -v $(pwd)/tf:/usr/src/tf \
    terraform-runner:latest \
    /bin/bash

  # Then inside the container:
  tf-plan.sh dev airbyte
  tf-apply.sh dev airbyte

  Important Notes

  1. Local State: The tf/airbyte/providers.tf:23-24 shows it's configured for local backend by default, so the Terraform state will be stored in tf/airbyte/terraform.tfstate on your local filesystem
  2. Backend Config: The tf/backends/dev.tfvars:1-11 file is essentially empty for local state usage - the scripts will still reference it but it has no effect
  3. Required Files: The scripts expect:
    - tf/environments/dev-common.tfvars (already exists)
    - tf/environments/dev-airbyte.tfvars (already exists)
  4. CA Certificate: The Dockerfile expects a cert at docker/terraform-runner/cert/cacert.crt - make sure this exists or modify the Dockerfile if not needed
  5. Workspace: The scripts automatically create/use a workspace named dev-airbyte
